{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b81c437f-faa8-4ca8-b02a-30f750fcace4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files combined successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of file names\n",
    "file_names = ['bank-additional.csv', 'bank-additional-full.csv']\n",
    "\n",
    "# Initialize an empty DataFrame to store the combined data\n",
    "combined_data = pd.DataFrame()\n",
    "\n",
    "# Loop through each file and append its data to the combined DataFrame\n",
    "for file_name in file_names:\n",
    "    # Read the CSV file into a DataFrame\n",
    "    current_data = pd.read_csv(file_name)\n",
    "    \n",
    "    # Append the current data to the combined data\n",
    "    combined_data = combined_data.append(current_data, ignore_index=True)\n",
    "\n",
    "# Write the combined data to a new CSV file\n",
    "combined_data.to_csv('combined_file_bank_additional.csv', index=False)\n",
    "\n",
    "print(\"Files combined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89369ab1-5258-4c51-b64b-7a21551f4b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of file names\n",
    "file_names = ['bank.csv', 'bankfull.csv']\n",
    "\n",
    "# Initialize an empty DataFrame to store the combined data\n",
    "combined_data = pd.DataFrame()\n",
    "\n",
    "# Loop through each file and append its data to the combined DataFrame\n",
    "for file_name in file_names:\n",
    "    # Read the CSV file into a DataFrame\n",
    "    current_data = pd.read_csv(file_name)\n",
    "    \n",
    "    # Append the current data to the combined data\n",
    "    combined_data = combined_data.append(current_data, ignore_index=True)\n",
    "\n",
    "# Write the combined data to a new CSV file\n",
    "combined_data.to_csv('combined_file_bank.csv', index=False)\n",
    "\n",
    "print(\"Files combined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc925778-fdec-415f-abdc-f522b6debafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data separated into columns and saved to updated_file_bank_additional.csv.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file with all data in a single column separated by semicolons\n",
    "input_file = 'combined_file_bank_additional.csv'\n",
    "df = pd.read_csv(input_file, sep=';', header=None)\n",
    "\n",
    "# Split the single column into separate columns using the semicolon as a delimiter\n",
    "df_split = df[0].str.split(';', expand=True)\n",
    "\n",
    "# Assuming the maximum number of columns is the length of the longest row\n",
    "max_columns = df_split.shape[1]\n",
    "\n",
    "# Rename the split columns with meaningful names (you can adjust this based on your needs)\n",
    "df_split.columns = [f'Column_{i+1}' for i in range(max_columns)]\n",
    "\n",
    "# Concatenate the split columns with the original DataFrame\n",
    "df_combined = pd.concat([df, df_split], axis=1)\n",
    "\n",
    "# Drop the original combined column if needed\n",
    "df_combined = df_combined.drop(columns=[0])\n",
    "\n",
    "# Write the result to a new CSV file\n",
    "output_file = 'updated_file_bank_additional.csv'\n",
    "df_combined.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Data separated into columns and saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c532e717-7bbc-45ef-b9a2-ca683f65c07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file with all data in a single column separated by semicolons\n",
    "input_file = 'combined_file_bank.csv'\n",
    "df = pd.read_csv(input_file, sep=';', header=None)\n",
    "\n",
    "# Split the single column into separate columns using the semicolon as a delimiter\n",
    "df_split = df[0].str.split(';', expand=True)\n",
    "\n",
    "# Assuming the maximum number of columns is the length of the longest row\n",
    "max_columns = df_split.shape[1]\n",
    "\n",
    "# Rename the split columns with meaningful names (you can adjust this based on your needs)\n",
    "df_split.columns = [f'Column_{i+1}' for i in range(max_columns)]\n",
    "\n",
    "# Concatenate the split columns with the original DataFrame\n",
    "df_combined = pd.concat([df, df_split], axis=1)\n",
    "\n",
    "# Drop the original combined column if needed\n",
    "df_combined = df_combined.drop(columns=[0])\n",
    "\n",
    "# Write the result to a new CSV file\n",
    "output_file = 'updated_file_bank.csv'\n",
    "df_combined.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Data separated into columns and saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "427ecbf1-b921-4aa7-8733-06caf830397f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in c:\\users\\denis\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\denis\\anaconda3\\lib\\site-packages (from imblearn) (0.9.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.1 in c:\\users\\denis\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\denis\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.20.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\denis\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.0.1)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\denis\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.6.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\denis\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy.stats import zscore, skew\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "409421d7-e299-45bf-a11d-a0dba97b1d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['job', 'marital', 'education', 'default', 'balance', 'housing', 'loan',\n",
       "       'contact', 'day', 'month', 'duration', 'campaign', 'pdays', 'previous',\n",
       "       'poutcome', 'y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('updated_file_bank.csv')\n",
    "\n",
    "df.isnull().sum().max()\n",
    "df.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9f5c8e5-8443-4a60-a4e4-8cab429a1272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 49731 entries, 0 to 49730\n",
      "Data columns (total 16 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   job        49731 non-null  object\n",
      " 1   marital    49731 non-null  object\n",
      " 2   education  49731 non-null  object\n",
      " 3   default    49731 non-null  object\n",
      " 4   balance    49731 non-null  int64 \n",
      " 5   housing    49731 non-null  object\n",
      " 6   loan       49731 non-null  object\n",
      " 7   contact    49731 non-null  object\n",
      " 8   day        49731 non-null  int64 \n",
      " 9   month      49731 non-null  object\n",
      " 10  duration   49731 non-null  int64 \n",
      " 11  campaign   49731 non-null  int64 \n",
      " 12  pdays      49731 non-null  int64 \n",
      " 13  previous   49731 non-null  int64 \n",
      " 14  poutcome   49731 non-null  object\n",
      " 15  y          49731 non-null  object\n",
      "dtypes: int64(6), object(10)\n",
      "memory usage: 6.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d9ee959-8cc0-4c88-91fc-4d7c1d2c79ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Handling Missing Values\n",
    "# Impute missing values with the mean\n",
    "df = df.fillna(df.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c2fdd49-6a9d-4a94-82f9-cbab96947996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Handling Skewed Data\n",
    "# Identify and apply log transformation to skewed numeric features\n",
    "numeric_features = df.select_dtypes(include=[np.number])\n",
    "skewed_features = numeric_features.apply(lambda x: x.skew()).sort_values(ascending=False)\n",
    "skewed_features = skewed_features[abs(skewed_features) > 0.5]\n",
    "for feature in skewed_features.index:\n",
    "    if (df[feature] > 0).all():  # Check if all values are positive\n",
    "        df[feature] = np.log1p(df[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5ced6486-0a8a-47be-9fc4-f90b2408aeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 44756 entries, 0 to 49729\n",
      "Data columns (total 16 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   job        44756 non-null  object \n",
      " 1   marital    44756 non-null  object \n",
      " 2   education  44756 non-null  object \n",
      " 3   default    44756 non-null  object \n",
      " 4   balance    44756 non-null  int64  \n",
      " 5   housing    44756 non-null  object \n",
      " 6   loan       44756 non-null  object \n",
      " 7   contact    44756 non-null  object \n",
      " 8   day        44756 non-null  int64  \n",
      " 9   month      44756 non-null  object \n",
      " 10  duration   44756 non-null  int64  \n",
      " 11  campaign   44756 non-null  float64\n",
      " 12  pdays      44756 non-null  int64  \n",
      " 13  previous   44756 non-null  int64  \n",
      " 14  poutcome   44756 non-null  object \n",
      " 15  y          44756 non-null  object \n",
      "dtypes: float64(1), int64(5), object(10)\n",
      "memory usage: 5.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 3. Handling Outliers\n",
    "# Identify and clip outliers using z-score\n",
    "z_scores = np.abs(zscore(df.select_dtypes(include=[np.number])))\n",
    "df_no_outliers = df[(z_scores < 3).all(axis=1)]\n",
    "\n",
    "# Display information about the cleaned dataset\n",
    "print(\"\\nCleaned Dataset Info:\")\n",
    "print(df_no_outliers.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "326f0a27-de01-4b62-b40a-dfb12f99d810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Handling Unbalanced Data using SMOTE\n",
    "# Assuming the target variable is 'target_variable'\n",
    "X = df_no_outliers.drop('y', axis=1)\n",
    "y = df_no_outliers['y']\n",
    "categorical_columns = X.select_dtypes(include=['object']).columns\n",
    "X_encoded = pd.get_dummies(X, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "# Apply SMOTE to balance the dataset\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed58f259-bb1a-409b-8259-72451d539dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'job', 'marital', 'education', 'default', 'housing', 'loan',\n",
       "       'contact', 'month', 'day_of_week', 'duration', 'campaign', 'pdays',\n",
       "       'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx',\n",
       "       'cons.conf.idx', 'euribor3m', 'nr.employed', 'y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('updated_file_bank_additional.csv')\n",
    "\n",
    "df1.isnull().sum().max()\n",
    "df1.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f514c21d-cc42-4b64-bd78-bc251fd28d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Handling Missing Values\n",
    "# Impute missing values with the mean\n",
    "df1 = df.fillna(df.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "817de97f-691b-4ce2-88d4-c4d10e22fba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Handling Skewed Data\n",
    "# Identify and apply log transformation to skewed numeric features\n",
    "numeric_features = df1.select_dtypes(include=[np.number])\n",
    "skewed_features = numeric_features.apply(lambda x: x.skew()).sort_values(ascending=False)\n",
    "skewed_features = skewed_features[abs(skewed_features) > 0.5]\n",
    "for feature in skewed_features.index:\n",
    "    if (df1[feature] > 0).all():  # Check if all values are positive\n",
    "        df1[feature] = np.log1p(df1[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "261b9f56-bc4e-4f54-bdb5-6810b429329f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 45206 entries, 0 to 49729\n",
      "Data columns (total 16 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   job        45206 non-null  object \n",
      " 1   marital    45206 non-null  object \n",
      " 2   education  45206 non-null  object \n",
      " 3   default    45206 non-null  object \n",
      " 4   balance    45206 non-null  int64  \n",
      " 5   housing    45206 non-null  object \n",
      " 6   loan       45206 non-null  object \n",
      " 7   contact    45206 non-null  object \n",
      " 8   day        45206 non-null  int64  \n",
      " 9   month      45206 non-null  object \n",
      " 10  duration   45206 non-null  int64  \n",
      " 11  campaign   45206 non-null  float64\n",
      " 12  pdays      45206 non-null  int64  \n",
      " 13  previous   45206 non-null  int64  \n",
      " 14  poutcome   45206 non-null  object \n",
      " 15  y          45206 non-null  object \n",
      "dtypes: float64(1), int64(5), object(10)\n",
      "memory usage: 5.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 3. Handling Outliers\n",
    "# Identify and clip outliers using z-score\n",
    "z_scores = np.abs(zscore(df1.select_dtypes(include=[np.number])))\n",
    "df1_no_outliers = df1[(z_scores < 3).all(axis=1)]\n",
    "\n",
    "# Display information about the cleaned dataset\n",
    "print(\"\\nCleaned Dataset Info:\")\n",
    "print(df1_no_outliers.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6d69e742-0c7c-4a52-ad7a-51d43654fe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Handling Unbalanced Data using SMOTE\n",
    "# Assuming the target variable is 'target_variable'\n",
    "X = df1_no_outliers.drop('y', axis=1)\n",
    "y = df1_no_outliers['y']\n",
    "categorical_columns = X.select_dtypes(include=['object']).columns\n",
    "X_encoded = pd.get_dummies(X, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "# Apply SMOTE to balance the dataset\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246f2cbd-acec-4e4a-8739-c4f01788990e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
